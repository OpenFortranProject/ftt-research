\section{Program Restructuring}

Major challenges face application programmers on exescale platforms.  Among
these challenges are of course performance, but in addition, programmers are
also challenged by the need for their applications to be maintainable and
portable across a wide variety of hardware platforms.  Unfortunately,
performance often is at odds with maintainability and portability.  For example,
hardware accelerators like GPUs are rapidly evolving and require separate coding
for each GPU vendor and even for a specific vendor's range of models.  A
synthetic cognition application at LANL has six different software versions just
to support a subset of the possible hardware combinations that exist today.

To meet these challenges we propose three main thrust areas:

1. Guidance to users on the choice of a programming model for their
applications.  This guidance will be based on an extensive analysis by proposal
team members on the models that would work well on existing and future exoscale
hardware architectures along with input from the user communities.  A high-level
programming model will provide application programmers with the ability to
incrementally raise the level of abstraction within their applications.  Only by
raising the level of abstraction can codes be retargeted by compilers across the
expected range of exoscale platforms.  Discussions with scientists in various
scientific disciplines (e.g., atmospheric science) suggest that programmers
would prefer that their codes just work \emph{as is} on exoscale platforms,
primarily because of the huge effort envolved in recoding a major scientific
code.  However experience on porting codes to LANL's petascale Roadrunner
computer suggests this may not be entirely possible, at least on early
encarnations of exoscale platforms.  Every code ported to Roadrunner was
completely rewritten to perform well on Roadrunner's many-core processors.  It
is our goal to provide user-guided refactoring tools to incrementally modify
codes in order to ease the porting effort for programmers, without a complete
hand recoding as was required for the initial applications that were ported to
Roadrunner.

2. Once a user or user community has chosen a programming model, tools can be
provided that restructure an application to adopt the chosen programming model.
These code refactorings need not be complicated.  For example, OpenMP uses
compiler directives to provide high-level information to the compiler.  Thus
analysis and refactoring tools guiding users in porting codes to an OpenMP
threading model would be appropriate.  Other tools supporting a data parallel
programming model would require a different set of tools.

3. Finally, given functions (or code segments) that have been restructured to
take advantage the chosen, high-level programming model, compiler tools are
needed to generate code for different system architectures.  We intent to
develop tools that are complimentary to and exploit a hardware vendor's C, C++,
OpenCL, and Fortran compilers.  We propose to develop source-to-source
transformational tools that take in high-level code and produce an intermidiate
code that can be further optimized by the vendor's backend compiler.

\subsection{Programming Models}

While it is impossible to predict the exact system architecture that an
exoscale system will have, we can anticipate the general direction that a
programming model must support.  We anticipate the need to support over 100K
nodes supported by a message passing communication system.  Each node can be
expected to contain a few 10s of heterogeneous cores with the possibility of
order 1K additional simple cores like those seen in existing GPUs.  Thus the
programming model must support this hierarchical distribution of
memory and processing elements.

Replicated programs using message passing with MPI is the current standard for
parallel programming using distributed nodes.  Parallel programmers have become
used to this environment and we expect to support this programming environment.  While
OpenMP is gathering acceptance, unfortunately there is currently not a
widely-adopted programming model that exploits on-node parallelism like MPI does
for distributed nodes.  Two evolving cross-platform standards for developing
multi-threaded applications are OpenMP and OpenCL.  We intend to develop tools
that support both.  OpenMP as a way to express parallelism directly within an
application and OpenCL as an intermediate language generated by our
source-to-source compiler tools.  OpenCL is too low-level a language to achieve our
code-maintainability goal but supports a data-parallel programming model as a
high level of abstraction.

\subsection{Source-Code Abstractions}

Since a majority of scientific codes are written in Fortran, a large effort will
be given to supporting the Fortran programming language.  Fortunately Fortran is
an ideal language for programming at a high level of abstraction, especially for
procedures chosen as targets for threaded architectures like GPUs, as it has a
well-developed array abstraction as part of the base language.  While not always
exploited in existing Fortran applications, most of these abstractions are
available in todays compilers (although not currently targeted for GPUs).
Fortran provides a data-parallel programming model much like CUDA and is well
suited for intermedicate code generation to, for example, OpenCL (or CUDA if it
becomes a standard supported by multiple vendors).  Many of these abstractions
can be extended to C and C++ by array abstractions and compiler directives.
Currently Intel is working on adding array extensions to their C compiler.

In Fortran a user can choose between several source-code abstractions for
exploiting parallelism.  The goal of this proposal is to provide tools transform
these abstractions to low-level code for final compilation.  Fortran supports a
data-parallel programming model with array syntax, e.g., C = A + B, where A, B,
C are arrays.  Or the user can explicitly use loop constructs like DO CONCURRENT
to expose parallelism (a Fortran 2008 addition) or by using similar constructs
available in OpenMP like, {\tt #pragma omp parallel for}.

\subsection{Locally-Oriented Programming}

We highlight the Fortran elemental source-code abstraction because it provides a
local orientation similar to that of OpenCL or CUDA kernel functions that were
designed to exploit streaming, highly-threaded architectures like GPUs.
Elemental functions are pure in that they are guaranteed to be free of side
effects such as I/O.  Consider a convolution where a 3x3 filter is applied to
individual pixel elements in a photograph to average of blur the original photograph.
A convolution is similar to stencil operations in computational fluid dynamics that
are used to obtain spatial derivatives of state variables.  Simple code is shown
with extentions to Fortran shown in capital letters.  The extensions could be
provided to C or Fortran functions via compiler directives rather than explicit 
language syntax.

\begin{verbatim}
    CONCURRENT real elemental function convolve(a, filter)
       real, intent(in) :: a, filter(3,3)
       real :: a_region(3,3)

       a_region = HALO(a, [1,1,1,1])
       convolve = sum( filter*a_region )

    end subroutine
\end{verbatim}

The {\tt convolve} function is called with
\begin{verbatim}
    A = convolve(A, filter)
\end{verbatim}
where {\tt A} and {\tt Avg} are arrays, but where the {\tt a} argument in the
{\tt convolve} definition is declared as a scalar.  The {\tt HALO} function
returns a copy of the 3x3 region of {\tt a} and its surrounding neighbors.  The
{\tt convolve} function is free of race conditions because of the copy semantics
of {\tt HALO} and the implied synchronization of the {\tt CONCURRENT} attribute,
whereby no output variables can be updated before all threads have completed
execution.  In addition, while any thread may load from an extended region
about \emph{its} element with the {\tt HALO} function, it may only store into
its own elemental location.

%% A note from a conversation with Matt regarding memory management
%%
\subsection{Memory Management}

The Fortran language has much tighter restrictions on aliasing than does C.
So unless a variable has the pointer or target attribute, it cannot be aliased.
Thus the compiler is able to agressively optimized for memory movement between
the CPU and accelerator.  However, because the design philosophy of coarrays
is that memory transfer between images can be expensive, the programmer must
explicitly transfer memory between images with explicit syntax with square
bracket notation, i.e., $a[1] = a[2]$.  So we allow the compiler to manage
memory within an image but require the use of {\tt halo_exchange} for the
transfer of halo memory between images.

\subsection{Code Restructuring Tools}

This example is provided in some detail as it highlights several of the tools
that are useful in aiding the user in the task of restructuring source code.
The goal of the restructuring is to transform it into a high-level, maintainable
form, that can be automatically transformed into a source form that can
ultimately be compiled into an executable on a particular hardware platform.  We
assume that the starting point is a traditional serial code using loop
constructs; the convolution example would require four nested loops.

\begin{itemize}

\item \emph{Code outliner.}  This tool would replace a block of code chosen by
the user with a function and a call to the function.  In the above example the
loops over the array elements would be moved to the elemental {\tt convolve}
function and replaced by a call to the newly created function.  The tool may
require input from the user as it completes its task (size of ghost/halo region
for example).

\item \emph{User modifications.} At this point the user may want to restructure
the outlined function {\tt convolve} by hand, by replacing the loops over the
filter with array syntax as shown in the example.

\item \emph{Insertion of memory placement hints.}  Any compiler generated code
will have to be conservative regarding where memory resides, whether on the
accelerator device or on the host CPU.  For the above example, memory for {\tt
A} will need to be copied to and from the device on entry and exit of function
{\tt convolve}.  However, memory for {\tt A} may already reside on the device
from a previous function invocation and therefore need not be copied.  Users may
provide hints to the compiler as to whether to copy the memory and if so, where
to begin the memory copy so that it can be done asynchronously with other work.
Tools will be created to guide the user in making this placement based on
dependence analysis.

\item \emph{Insertion of MPI communication.}  Memory placement hints can also be
used so that calls to MPI can be inserted into the code for the communication of
ghost cells between nodes.  Users may start with a version of the code that
already uses MPI, but the same information needed to generate OpenCL kernels can
be used to generate MPI communication; off-node memory is just another level in
the memory hierarchy.

%%% NOTE, I can provide a figure here showing the memory hierarchy.

\item \emph{Insertion of OpenMP directives.}  As an alternative to outlining a
function (the opposite of inlining) a tool will be created to restructure user
code by inserting OpenMP directives to indicate that loop iterations can be
performed concurrently, for example.  This tool will provide guidance to the
user as to where it may be appropriate to place these directives (and why) but
the ultimate responsibility for where and what directives to be used is a user's
responsibility.

\item \emph{Replace with DO CONCURRENT.}  Another alternative to code outlining
is to replace loops in Fortran with the DO CONCURRENT construct.  As with
compiler directives, this restructuring would require user input and can also
suggest loops that are candidates for converting.

\end{itemize}

At this point it is assumed that the user has finished restructuring the
original source into a form such that that parallelism is exposed to the
source-to-source compiler through language syntax or compiler directives.  This
is the high-level, maintainable, and portable source form that is stored in CVS
repository.  Tools are also required that can take the high-level source and
retargets it to a specific hardware platform.  These tools produce a lower-level
source, through source-to-source transformations, that a vendor's compiler will
use to generate executable code.  While described as separate tools, they will
likely be aggragated into a single tool.

The transformations may produce code for following targets (a combination of
language and hardware platform): 1. Serial version to be run on a single core.
This is necessary because some language constructs like DO CONCURRENT may not be
available for a period of time; 2. OpenCL version optimized for GPUs; 3. OpenCL
for optimzed for multiple CPU cores; and 4. OpenMP version for final compilation
with an OpenMP compiler.


\begin{itemize}

\item \emph{Thread initialization.} This tool generates code for calls to the
thread runtime library to initialize all thread devices and thread memory.

\item \emph{Code inliner.} This tool inlines calls to elemental functions,
effectively undoing the effect of the outliner. It is not needed for Fortran as
the compiler already performs this task.

\item \emph{Thread execution.} This tool generates code for execution of an
elemental or kernel function by a thread runtime library like OpenCL.

\item \emph{Memory transfer.}  This tool generates code for all memory transfer
and synchronization points.

\end{itemize}


Programming Models
------------------

Fortran currently supports:

1. Array syntax: e.g., C = A + B, where A, B, C are arrays.  Note that this is implicitly a loop structure, but that no loop indices need be provided.  Also the programmer need not specify where in memory these arrays reside.  Thus this high level syntax allows the compiler more freedom in both memory placement (even across distributed memory nodes) and in runtime code execution (individual array element may be computed by different hardward threads).  This is a simple example and it is a research question as to what compiler directives would be useful for memory placement and other directions to the compiler for efficient code generation.

2. Pure procedures:  Fortran has syntax for specifying procedures that have no side effects during execution.  Specifying code that is side-effect free code is important information to provide to the compiler so that it can generate efficient multi-threaded code.

3. Pure elemental procedures: Fortran has syntax for specifying procedures that take only scalar arguments, but may be applied across array elements.  Elemental procedures are ideal for writing code to be executed within a hardware thread.  They resemble OpenCL kernels, but are simpler because they leave all indexing up to the compiler.

We have determined that additional syntax is needed, in addition to the three language features described above, to allow programmers the ability to express code in Fortran to be targeted for multi-threaded hardware architectures like GPUs.  This additional syntax is provided by functions that return a copy of a small region of memory surrounding an array element (as seen within an elemental procedure) and with functions for thread synchronization.  This additional syntax will allow pure procedures to perform stencil and other convolution-like operations on a copy of memory, synchronize, then store the computed results back to the array element associated with the given thread.

In addition Fortran has syntax like the target attribute the specifies when variables can be aliased.  This allows for much easier program analysis as the compiler knows that ordinary variables cannot be aliased.  Fortran also has excellent facilities for interoperability with C so that programming in a mixed language invironment is easily accomplished, including interoperability with native Fortran arrays.
