\section{Introduction}

This paper presents a compiler-level approach for targeting a single
program to multiple fundamentally different low level execution and
programming models while allowing the application programmer to adopt
a single high level programming model.  We show that features of
Fortran 90 for data parallel programming are well suited to automatic
transformation to generate code specifically tuned for different
low-level programming models such as OpenCL and CUDA.  For algorithms
that can be easily expressed in terms of whole array, data parallel operations,
writing code in Fortran and transforming it automatically to specific low
level implementations removes the burden from the programmer of working
with tedious, error prone, low-level tools.

1. Goal is to write once, transform many. Otherwise potentially reprogram for every architecture.
2. Goal is to code for readability and maintainability, not performance
3. This goal requires expression in a high-level language.
4. But language must be simple enough for compiler to analyze.
5. Thus ideal if language maps well to accelerator architectures.
6. Data parallel constructs in Fortran 90 are chosen.
7. Up to 65 times speed up measured on automatically transformed code.

At Los Alamos National Laboratory (LANL) we have a wide variety of computer
platforms from which to choose.  The common platform is made up of clusters of
compute nodes with standard multi-core processors.  Some nodes also have
accelerators that range from the IBM Cell processor (Roadrunner) to a variety
of GPUs from NVIDIA and AMD.  Some nodes have hardware with vector
instructions and others do not. The peak
performance of the accelerated nodes range from 200+ GFlops for the IBM Cell/B.E.
processor to XXX for NVIDIA Fermi. %% Fermi has another name

%% NEW - CER
However, the performance that these accelerator architectures offer comes at a
cost as processor architectures are trending toward multiple cores with
instances of integrated accelerator units and less of a reliance on memory
caches.  These changes place a heavy burden on application programmers as they
are forced to adapt to the new systems.  An especially challenging problem
faced by application programmers is not only how to program to these new
architectures (considering the massive scale of concurrency available), but
also how to design programs that are portable across the changing landscape of
computer architectures.  The fundamental question we address is what
programming model and language constructs are best suited for these platforms.

%% NEW - CER
%%%We will examine how the existing data-parallel constructs in Fortran can be combined with coarrays or MPI to provide effectively a new parallel programming language, one that is evolutionary in nature and provides complete compatibility with existing applications and libraries.  Data parallelism is a high-level abstraction that is, at the same time, both easier to program and gives the compiler more leeway (if fully exploited) in retargeting a program to different computer architectures.

A data parallel programming model is well suited to emerging
architectures that are based on either vector processing or massively
parallel collections of simple cores.  A popular trend for programming
systems such as GPU accelerators is the CUDA language from NVidia, or
new languages like Chapel from Cray.  A less intrusive approach that
is well suited to legacy applications and languages are language
extensions or libraries that allow programmers to avoid adopting
entirely new languages.  The recent OpenCL specification is intended
to support this programming model, as are directive-based methods such
as OpenMP or the Accelerator programming model from the Portland
Group~\cite{pgi10accelerator}.

The problem with many of these choices is that they expose too much detail
about the machine architecture to the programmer.  
This is particularly true of CUDA and OpenCL.  In CUDA, programmers must
adapt their codes to fit the threading model used by NVidia GPUs, while
OpenCL requires programmers to provide specially tuned versions of
their code for different classes of machine.  In both cases, the programmer
is also responsible for explicitly managing memory, including staging of
data back and forth from the host CPU and the accelerator device memory.
While these models have been attractive as a method for early adopters
to utilize these new architectures, they are less attractive to programmers
who do not have the time or resources to manually port their code to every
new architecture and programming model that emerges.

At this point in time it is not really possible to write once and
run efficiently on the wide variety of computer platforms we have
available.  For some classes of applications, we believe that this goal
is possible using language constructs already present in a popular
mainstream scientific programming language -- Fortran.

\subsection{Approach}

This paper addresses this issue by examining features in Fortran that
allow programmers to express algorithms at a very high level but yet
can be transformed by a compiler to run efficiently on a wide variety
of platforms.  In particular we consider nodes of GPUs.

We demonstrate that the array syntax of Fortran maps surprisingly well
onto GPUs when transformed to OpenCL kernels.  These Fortran language
features include pure and elemental functions and array constructs
like where and shift.  In addition we add a few functions that enable
a program to be deployed on machines with a hierarchy of processing
elements, such as nodes employing GPU acceleration, \emph{without
  requiring explicit declaration of parallelism within the program.}
In addition the program uses entirely standard Fortran so it can be on
a single core without concurrency.

We provide (via Fortran interfaces in the ForOpenCL library) a mechanism to
call the C OpenCL runtime and enable Fortran programmers to access OpenCL
kernels.  Transformations are supplied that provide a mechanism for converting
Fortran procedures written in the Fortran subset described in this paper to
OpenCL kernels.  We use the ROSE compiler infrastructure \cite{} to develop
these transformations.  ROSE uses the Open Fortran Parser \cite{} to parse
Fortran 2008 syntax and can output (unparse) to OpenCL.  Since ROSE's
intermediate representation (IR) was constructed to represent multiple
languages, it is relatively straightforward to transform high-level Fortran IR
nodes to OpenCL nodes.  Furthermore, the Fortran array syntax maps directly to
one, two, and three-dimensional thread groups in OpenCL.

Transformations for general Fortran procedures are not provided.  Furthermore,
a mechanism to transform the calling site to automatically invoke
OpenCL kernels is not provided at this time.  While it is possible to
accomplish this task within ROSE, it is considered outside the scope of this
paper.

We examine the performance of the Fortran data-parallel abstraction when
transformed to OpenCL to run on GPU architectures.  Since single node performance
is often given as a reason for not using data-parallel constructs within Fortran,
we consider the performance of serial data-parallel codes compared with the
usage of explicit loop constructs.

We study automatic transformations and the performance for an
application example that is typical of many applications that are based on
finite-difference or finite-volume methods in computational fluid dynamics
(CFD).  The example is a simple shallow water model in two dimensions using
finite volume methods with .... for time update.

%% NEW - CER - copied from proposal
An initial study was made for an important procedure in PAGOSA,
production code at LANL completely written in data-parallel
Fortran.  We investigated automatically transforming this code
to run on LANL's Petaflop Roadrunner computer (a hybrid mixture of Opterons
and IBM Cell processors).  We demonstrated that a source-to-source compiler
can automatically vectorize and parallelize (both within the Cell processor
and across nodes) a small section of this code.  Preliminary results showed a
12 times performance gain on the Cell processor over a traditional single core
processor.  A factor of 9 is gained from vectorizing and parallelizing the
code to run on the Cell and the rest is gained from automatically overlapping
inter-node communication with computation on the Cell.

%%We study automatic transformations of Fortran for two applications examples.  The first is a simple shallow water model in two dimensions using finite volume methods with .... for time update.  The second is based on the primary compute kernels from PetaVision, a high-performance, neural science framework that is used in models of visual cortex.

%% To simplify things, lets save PetaVision for the local stuff as it requires convolutions

\subsection{Why Fortran?}

%% NEW - CER
Fortran \footnote{Originally named FORTRAN for FORmula TRANslation language.}
is the oldest high-level programming language, and was developed to facilitate
the translation of math formulae into machine code. Fortran was the first
major language to use a compiler to translate from a high-level program
representation to assembly language. Due to its age, it carries certain arcane
baggage that later programming languages have evolved away from. As a result,
Fortran has fallen into disfavor in certain programming circles.

%% NEW - CER
However, with the introduction of Fortran 90 (and later revisions to the
standard), Fortran is a truly modern programming language. Much of the arcane
baggage associated with it is actually no longer part of the language used by
modern programmers, and exists in a deprecated form simply to support legacy
codes. It is now modular and has many object-oriented features. Furthermore,
Fortran should be of interest to those studying parallel programming because
of its functional and data parallel constructs and because of the coarray
notation introduced after Fortran 2003. Unlike languages like C and C++, which
are considered to be more modern, Fortran has become a truly parallel language
with features added to recent language standards.

Yet, a likely question that one may pose is ``\emph{Why Fortran and not a more
  modern language like X?}''  The recent rise in interest in concurrency and
parallelism at the language level driven by multicore CPUs and manycore
accelerators has driven a number of new language developments, both as novel
languages and extensions on existing ones.

%% NEW - CER
So perhaps it is time to replace Fortran with yet another computer language.  The
problems with replacing Fortran with an entirely new language are two fold:
the economics of replacing the existing application base and the difficulty in
obtaining programmer acceptance.  It is estimated that replacing a major
production application at Los Alamos National Laboratory would cost between 50
and 150 million dollars.  In terms of programmer acceptance, there is always
the "chicken and egg problem": programmers won't use a new language until they
can expect good performance across a variety of platforms, and compiler
vendors can't afford to produce quality compilers until there is a reasonable
expectation of a market.

For scientific users, these new languages and language extensions
present a challenge: how do developers effectively use them while
avoiding rewriting their code and potentially growing dependent on a
transient technology that will vanish tomorrow?

%% NEW - CER
History has also shown that an investment in rewriting code does not guarantee
success either, as seen in an effort at LANL to modernize a legacy Fortran
code with the the newer C++ POOMA framework.  This massive overhaul effort led
to a code that was both slower and less flexible than the original Fortran
\cite{}.  Similar experiences have occurred in the past, notably during the
devlopment of the Sisal language.

Fortran is unique in that it has contained language features that are
well suited to modern architectures for a number of years.  This
should be unsurprising --- Fortran was a primary language used to
target systems such as the vector supercomputers and massively
parallel systems of the 1970s and 1980s.  These are the systems in
which architectural features were developed that have led to single
chip high performance architectures of interest today.  Given that
these new systems have features very similar to their predecessors, it
is clear that the language features within Fortran for them are still
relevant.
