%
% I'm adding an outline here so I can see how everything fits together

%
% \title{Exploiting First-Class Arrays in Fortran for Accelerator Programming}
%

%
% \abstract
%

%
% \section{Introduction}
%    \subsection{Approach}
%    \subsection{Why Fortran?}
%    \subsection{Comparison to Other Languages}
%

%
% \section{Programming model}
%    \subsection{Fortran Syntax}   
%       \subsubsection*{Array notation}
%       \subsubsection*{Elemental functions}
%       \subsubsection*{Pure procedures}
%       \subsubsection*{Shift functions}
%       \subsubsection*{Regions}
%    \subsection{New Functions}
%    \subsection{Parallelism}
%    \subsection{Limitations}
%

%
% \section{Source-To-Source Transformations}
%    \subsection{ForOpenCL}
%       \subsubsection{array syntax}
%       \subsubsection{where construct}
%    \subsection{New functions}
%       \subsubsection{region}
%    \subsection{Static Analysis}
%       \subsubsection{Analysis not required}
%    \subsection{Simplifying Assumptions}
%

%
% \section{Shallow Water Model}
%    \subsection{Equations}
%

%
% \section{Performance}
%

%
% \section{Conclusions}
%


\documentclass[10pt, conference, compsocconf]{IEEEtran}

\usepackage{cite}
\usepackage{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{array}
\usepackage{url}


\title{Exploiting First-Class Arrays in Fortran for Accelerator Programming}

%%
%% reorder appropriately later
%%
\author{\IEEEauthorblockN{Matthew J. Sottile}
\IEEEauthorblockA{Galois, Inc.\\
421 SW 6th Ave. Suite 300 \\
Portland, OR 97204\\
Email: matt@galois.com}
\and
\IEEEauthorblockN{Craig E Rasmussen}
\IEEEauthorblockA{Los Alamos National Laboratory\\
CCS-7, MS B287\\
Los Alamos, NM\\
Email: rasmussn@lanl.gov}
}

\begin{document}

\maketitle

\begin{abstract}
Emerging architectures for high performance computing often are well
suited to a data parallel programming model.  This paper presents a
simple programming methodology based on existing languages and compiler
tools that allows programmers to take advantage of these systems.
We will work with the array features of Fortran 90 to show how this
infrequently exploited, standardized language feature is easily
transformed to lower level accelerator code.  Our transformations are
based on a mapping from Fortran 90 to C++ code with OpenCL extensions.
\end{abstract}

\input{intro}


\section{Comparison to Other Languages}

%% Matt you should provide an historical (and current) context here


Techniques based on whole-array programming being mapped to high performance,
parallel implementations are not new.  

Similar to ZPL, CM5 stencil compilers, CMFortran stuff.

%% cites: ZPL, CM5 stencil compilers POOMA, HPL OpenMP, CUDA

% put something here on: regions, array notation, ...

\section{Programming Model}

The static analysis and source-to-source transformations used are very
basic but require the programmer to use a subset of Fortran
that employs a data-parallel programming model.  In particular, it
encourages use of language features that were introduced and
standardized in the Fortran 90 language specification.  In this
section we describe the set of Fortran 90 features that our analysis
and transformation method are based on.

This paper uses four simple features of Fortran that form a language subset
that does not require any language extensions and can be easily transformed
to a lower-level implementation in either CUDA or OpenCL.

\subsubsection*{Array notation}

Fortran 90 introduced a rich array syntax that allows programmers to
write code that is in terms of whole arrays or subarrays, with data
parallel operators to compute on the arrays.  This has the benefit of
avoiding explicit looping in the code and maintaining a high-level
style that is closer to the original mathematical specification of the
problem being solved.  More importantly from a compilation
perspective, this defers decisions about how to implement these
whole-array operations to a compilation tool.  When faced with a novel
architecture such as modern GPUs that are ideally suited to data parallel
programming models, the fit between Fortran arrays and these systems is
quite clean.

%% NEW - CER
Arrays are first class citizens in Fortran as they are part of the language
type system rather than just a pointer to raw memory as in C. A Fortran array
contains metadata associated with it that describes the array's shape and
size. This metadata is lacking in other languages, such as C, C++, and Java.
Even in the case where multidimensional array data structures are available
(such as in C++ or Java), they exist outside the language standard, limiting
the ability of the compiler to analyze them fully. The existence of this
metadata allows the use of array notation whereby explicit indexing of arrays
is not required. For example, if {\tt A}, {\tt B}, and {\tt C} are all arrays
of the same rank and shape and {\tt s} is a scalar, then the statement

\begin{verbatim}

 C = A + s*B

\end{verbatim}

results in the sum of elements of {\tt A} and {\tt s} times the elements of
{\tt B} being stored in the corresponding elements of {\tt C}. The first
element of {\tt C} will contain the value of the first element of {\tt A}
added to the first element of {\tt c*B}.  Note that no explicit iteration over
array indices is needed and that the individual operators, plus, times, and
assignment are applied by the compiler to individual elements of the arrays
independently.  Thus the compiler is able to spread the computation in the
example across any hardware threads under its control.

%% NEW - CER
%%% The use of array notation allows one to program in a data-parallel subset of Fortran. This style of programming makes use of array notation and pure and elemental functions to operate on array elements.

\subsubsection*{Elemental functions}

An elemental function consumes and produces scalar values, but can be applied
to variables of array type such that the function is applied to each and every
element of the array.  This allows programmers to avoid explicit looping and
instead simply state that they intend a function to be applied to every
element of an array in parallel, deferring the choice of implementation
technique to the compiler.  Elemental functions are intended to be used for
data parallel programming, and as a result must be side effect free (or,
\emph{pure}).

%% NEW - CER
%%% Because elemental functions return scalar values and are free from side effects, the compiler is free to distribute the computation over any hardware processing elements available to it, such as the multiple cores and vector units on an Intel or AMD processor or the Synergistic Processing Units on an IBM Cell processor.

%% NEW - CER
An example of shown above using array syntax could be refactored to use
the elemental function,

\begin{verbatim}
   pure elemental real function elem_example(a, b, s)
      real, intent(in) :: a, b, s
      elem_example = a + s*b
   end function
\end{verbatim}

and called with

\begin{verbatim}
   C = elem_example(A, B, s)
\end{verbatim}

Note that while {\tt elem\_example} is defined in terms of purely scalar
quantities, it can be \emph{applied} to arrays as shown.

While this may seem like a trivial example, such simple functions may be
composed with other elemental functions to perform powerful computations,
especially when applied to arrays.  ForOpenCL transforms elemental functions to inline OpenCL
functions.  Thus there is no penalty for usage of elemental functions and provide
a convient mechanism to express algorithms in simpler segments.

Also note that the plus and times operators shown in the data-statement
example can be seen as an elemental functions, as plus and times are defined
in terms of scalars but can be applied to whole arrays and can return an array
result.

\subsubsection*{Pure procedures}

Pure procedures, like elemental functions, must be free of side effects.
(athough it may change the value of array elements in an array argument,
unlike elemental arguments which must have the intent(in) attribute).
The absence of side effects removes ordering constraints from the compiler
allowing it to invoke pure functions out of order.  Procedures and functions
of this sort are also common in pure functional languages like Haskell,
and are exploited by compilers in order to emit parallel code automatically
due to their suitability for compiler-level analysis.

Since pure procedures don't have side effects they a candidates for running on
accelerators in OpenCL.  Currently ForOpenCL transforms pure procedures to
OpenCL kernels that emph{do not} call other procedures, except for elemental
functions.

\subsubsection*{Shift functions}

Many array-based algorithms require the same operation to be performed
on each element of the array using the value of that element and some small
set of neighboring cells.  Often programmers implement these operations that
are local to each element within the inner-loop of a set of nested FOR-
or DO-loops using offsets relative to the current array index.  An alternative
to this local-view of the algorithm is to take a global view and write the
algorithm in terms of the whole array.  For example, consider a 1D array in
which we wish to subtract the $(i-1)$th element from the $i$th for all
elements.  One way to look at this is that we are subtracting the entire array
shifted by one element from itself.

Fortran provides a set of shifting operators that allow programmers to
define operations based on shifted arrays.  These intrinsic operators take
an array, a dimension, and the amount by which it should be shifted (using
the sign to indicate direction).  By defining operations on entire arrays
based on a global view of them shifted relative to each other, programmers can
avoid explicit looping and potentially tricky index arithmetic.  Furthermore,
analysis of the extent of the set of shifted arrays in a given expression
allows analysis tools to determine the amount of temporary or buffer storage
necessary to hold intermediate values during whole array operations.  With
explicit loops, programmers must maintain this temporary storage manually.

\subsubsection*{Regions}

Borrowing from ZPL, we introduce the concept of regions to Fortran.  In
Fortran, regions are expressed as pointers to a subsection of an existing array.
Regions may refer to an interior portion of an array or the entire array.

The use of regions implies the use of the halo (or ghost) cell pattern 
where the size of an array is increased to provide extra array elements
surrounding the interior portion of the array.  
%parlab.eecs.berkeley.edu/wiki/_media/patterns/ghostcell.pdf

Regions are similar to the shift operator as they can be used to reference
portions of the array that are shifted with respect to the interior portion.
However, unlike the shift operator, regions are not expressed in terms of
boundary conditions and thus don't explicitly \emph{require} a knowledge of
not the application of boundary conditions locally.  Thus, as will be shown
below, regions are more suitable for usage by OpenCL thread groups which
access only local subsections of an array stored in global memory.

Unfortunately, as regions functions return pointers to arrays, they cannot
be used in pure procedures in current Fortran.  We relax this constraint in
ForOpenCL for region pointers only.

\subsection{New Functions}

% TODO - Craig - describe these functions
%   region
%   interior (can be written in terms of regions) but also specifies the thread
%       group domain)
%   more ...
%


\subsection{Parallelism}

% TODO - Matt - do you want a try at this.

% need to describe processor hierarchy of nodes and accelerators
% need to describe memory hierarchy as well in regards to global array view,
% node array view (with a copy is GPU global memory) and local GPU memory which
% further subdivides the node array memory

% figures on slicing and dicing of memory region go here


There are several advantages to this style of programming using array syntax, shifts, regions,
and pure and elemental functions:

\begin{itemize}
	\item There are no loops or index variables to keep track of.  Off by one
index errors are a common programming mistake.
        \item The written code is closer to the algorithm, easier to understand, and is usually substantially shorter.
	\item Semantically the intrinsic functions return arrays by value (though not regions).  This is usually what the algorithm requires.
	\item Because pure and elemental function are free from side effects, it is easier for a compiler to schedule the work to be done in parallel.
\end{itemize}

%% NEW - CER
An example of this style of programming in Fortran is shown in

\begin{verbatim}

  Bz = Bz + dt * (cshift(Ex,dim=2,shift=+1) - Ex) / dy  &
          - dt * (cshift(Ey,dim=1,shift=+1) - Ey) / dx

\end{verbatim}

%% NEW - CER
This example is a solution to Maxwell's equations for the $z$ component of the
magnetic field using Fortran array syntax.  Note that there are no explicit
loops in this example.  The operators {\tt +}, {\tt -}, and {\tt *} are
applied to all of the elements of the three-dimensional arrays {\tt Bz}, {\tt
  Ex}, and {\tt Ey}, individually.  This is why data parallelism has been
called collection-oriented programming by
Blelloch~\cite{blelloch90,rajopadhyedidlacs}.  As the {\tt cshift} function
and the array-valued expressions all semantically returns a value, this style
of programming is also similar to functional programming (or value-oriented
programming). %%~\cite{simonpeytonjones}).  The heart of the solution to
Maxwell's equations is the statement shown in Listing~\ref{lst:dpexample} and
five similar, simple equations.

%% NEW - CER
%%% The power in this notation is that the compiler is free to distribute the computation in these expressions over any hardware processing elements available to it, such as the vector units on an Intel or AMD processor or the Synergistic Processing Units on an IBM Cell processor.  If the arrays are declared as coarrays, this includes spreading the computation over \emph{all of the nodes in   a cluster as well.}  In this case, communication occurs within the \texttt{cshift} functions, though the compiler is free to overlap communication and computation by scheduling the communication early and performing the computation on the interior of the arrays while waiting for the communication to complete.

%% NEW - CER
Complete and very concise and elegant programs can be built with procedures
similar to the example shown above. To aid this effort, Fortran
supplies intrinsic functions like the array constructors (\texttt{CSHIFT},
\texttt{EOSHIFT}, \texttt{MERGE}, \texttt{TRANSPOSE}, ...), the array location
functions (\texttt{MAXLOC} and \texttt{MINLOC}), and the array reduction
functions (\texttt{ANY}, \texttt{COUNT}, \texttt{MINVAL}, \texttt{SUM},
\texttt{PRODUCT}, ...).  To this set we add region functions described above.

%% NEW - CER
This style of programming meets the requirements we have set for a programming
model for developing applications suitable for acceleration.  It allows the programmer to
program at a very-high level of abstraction while providing the compiler with
maximum flexibility in targeting the application for a particular hardware
architecture.  The data parallel programming model simultaneously meets the
seemingly conflicting goals of maintainability, portability, and performance.

%% NEW - CER
Unfortunately, this style of programming has never really caught on because
when Fortran 90 was first introduced, performance was relatively poor and thus
programmers shied away from using array syntax (even now, some are actively
counseling against its usage because of performance issues~\cite{Cray-SC07}).
Thus the Fortran community was caught in a classic ``chicken-and-egg''
conundrum: (1) programmers didn't use it because it was slow; and (2)
compilers vendors didn't improve it because programmers didn't use it.

%% NEW - CER
A goal of this paper is to demonstrate that parallel programs written in
this style of Fortran are maintainable and can achieve good performance on a
variety of processor architectures, including possibly, future terascale
architectures from Intel (such as the XXX architecture).

\subsection{Limitations}

Only Fortran procedures are transformed into OpenCL kernels.  The programmer
must currently explicitly call these kernels from Fortran using the ForOpenCL
library described below.  It is also possible using ROSE to modify the calling
site so that the entire program can be transformed but this functionality is
outside the scope of this paper.  Here we specifically examine transforming
Fortran procedures to OpenCL kernels.

Only elemental functions may be called from kernel functions.  These include
fortran functions that have an OpenCL analog and user-defined elemental functions.

Array sizes must be multiples of the local kernel size,
{\tt get\_local\_size(0)*get\_local\_size(1)}.  This may be relaxed in the future.


\section{Source-To-Source Transformations}

% TODO - craig - I need to go through this section and replace PetaVision
% with the shallow water code

\subsection{ForOpenCL}

ForOpenCL is library of Fortran modules.  It contains Fortran 2003 interface
descriptions that allow language interoperability with the C OpenCL runtime.

\section{Transformation examples}

This sections show short Fortran code examples and OpenCL equivalent.
The notation uses uppercase for arrays and lowercase for scalar quantities.
Interior array subsections are denoted by an i succeeding the array, e.g. Ai
is the interior region of array A.

\subsubsection{array syntax}

PetaVision updates the action potential with the statement,

\begin{verbatim}
   V = V - iA*(V - v_rest)                  !! Fortran

   V[l] = V[l] - iA[lex]*(V[l] - v_rest);   // OpenCL equivalent
\end{verbatim}

\subsubsection{where construct}

A neuron in PetaVision fires (activity is set to 1)
whenever the action potential is greater than some threshold.
This is easily expressed with a where construct.

\begin{verbatim}
   where (V > Vth)                        !! Fortran
      iA = 1
   elsewhere
      iA = 0
   end where

   iA[lex] = (V[l] > Vth[l]) ? 1 : 0;     // OpenCL equivalent
\end{verbatim}

\subsection{New functions}

%\subsubsection{transfer_halo}
\subsubsection{region}

One of the state variables in the shallow water code is H, effectively the
height of the water.  This variable has a halo (ghost-cell region) surrounding
the interior of the grid to handle boundary conditions.  When using
MPI, the halo regions contains values from adjacent processors that must be
updated with new values at each time step.  This is accomplished with the
{\tt transfer\_halo} function.  The shallow water code assumes a five point stencil
so the state variables are extended by 2 in each dimension (e.g., by one to the
left, right, up, and down).

\begin{verbatim}
halo = [1,1,1,1]
iH => region(H, halo, transfer=false)
\end{verbatim}

\section{Static Analysis}

1. Dependence analysis to insert thread group barriers

\begin{verbatim}
   barrier(CLK_LOCAL_MEM_FENCE)
\end{verbatim}

2. subsection variables associated with array variables

\subsection{Analysis not required}

1. loop fusion
2. removal of array temporaries (shifts)

\section{Performance}

\section{Conclusions}

%% Explain what we have accomplished (copied from proposal)

%% NEW - CER
The sheer complexity of programming for clusters of many or multi-core
processors with tens of millions threads of execution make the simplicity of
the data parallel model attractive.  Furthermore, the increasing complexity of
todays applications (especially when convolved with the increasing complexity
of the hardware) and the need for portability across hardware architectures
make a higher-level and simpler programming model like data parallel
attractive.

%% NEW - CER
The goal of this work has been to exploit source-to-source transformations that
allow programmers to develop and maintain programs at a high-level of
abstraction, without coding to a specific hardware architecture.
Furthermore these transformations allow multiple hardware architectures
to be targeted without changing the high-level source.  It also removes the
necessity for application programmers to understand details of the accelerator
architecture or to know OpenCL.

% Summarize results


\section{Junk}

\cite{chamberlain04zpl, roth97stencils}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,foropencl}

\end{document}
