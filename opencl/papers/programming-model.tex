\section{Programming Model}

The static analysis and source-to-source transformations used are very
basic and only require the programmer to use a language subset that
employs a data-parallel programming model.  In particular, it
encourages use of four language features introduced in Fortran 90 ---
array notation, shift functions, elemental functions, and pure
procedures.  From these language constructs, we are able to easily
transform them to a lower-level CUDA or OpenCL implementation.

%This paper uses four simple features of Fortran that form a language subset
%that does not require any language extensions and can be easily transformed
%to a lower-level implementation in either CUDA or OpenCL.

\subsubsection*{Array notation}

Fortran 90 introduced a rich array syntax that allows programmers to
write statements in terms of whole arrays or subarrays, with
data parallel operators to compute on the arrays.
Array variables can be used in expressions
based on whole-array operations.  For example, if {\tt A}, {\tt B},
and {\tt C} are all arrays of the same rank and shape and {\tt s} is a
scalar, then the statement

%% Deleted by CER
%%This has the benefit of avoiding explicit looping in the code and maintaining a high-level style that is closer to the original mathematical specification of the problem being solved.  More importantly from a compilation perspective, this defers decisions about how to implement these whole-array operations to a compilation tool.  Sophisticated loop analysis to identify parallelism within the sequential loop code are not necessary.  When faced with a novel architecture such as modern GPUs that are ideally suited to data parallel programming models, the fit between Fortran arrays and these systems is quite clean.

%% Deleted by CER
%% A Fortran array combines a base pointer to memory with metadata that describes the array shape, stride and size. It should be noted that other languages that provide rich array data types and whole array operations may also be suitable targets for the transformations described in this paper.  

{\small
\begin{verbatim}
 C = A + s*B
\end{verbatim}
}

\noindent results in the element-wise sum of {\tt A} and {\tt s} times
the elements of {\tt B} being stored in the corresponding elements of
{\tt C}. The first element of {\tt C} will contain the value of the
first element of {\tt A} added to the first element of {\tt c*B}.
Note that no explicit iteration over array indices is needed and that
the individual operators, plus, times, and assignment are applied by
the compiler to individual elements of the arrays independently.  Thus
the compiler is able to spread the computation in the example across
any hardware threads under its control.

%% NEW - CER
%%% The use of array notation allows one to program in a data-parallel subset of Fortran. This style of programming makes use of array notation and pure and elemental functions to operate on array elements.

\subsubsection*{Elemental functions}

An elemental function consumes and produces scalar values, but can be applied
to variables of array type such that the function is applied to each and every
element of the array.  This allows programmers to avoid explicit looping and
instead simply state that they intend a function to be applied to every
element of an array in parallel, deferring the choice of implementation
technique to the compiler.  Elemental functions are intended to be used for
data parallel programming, and as a result must be side effect free and
mandate an {\tt intent(in)} attribute for all arguments.

%% NEW - CER
%%% Because elemental functions return scalar values and are free from side effects, the compiler is free to distribute the computation over any hardware processing elements available to it, such as the multiple cores and vector units on an Intel or AMD processor or the Synergistic Processing Units on an IBM Cell processor.

%% NEW - CER
For example, the basic array operation shown above could be refactored into
an elemental function,

{\small
\begin{verbatim}
  pure elemental real function foo(a, b, s)
    real, intent(in) :: a, b, s
    foo = a + s*b
  end function
\end{verbatim}
}

\noindent and called with

{\small
\begin{verbatim}
  C = foo(A, B, s)
\end{verbatim}
}

Note that while {\tt foo} is defined in terms of purely scalar
quantities, it can be \emph{applied} to arrays as shown.  While this
may seem like a trivial example, such simple functions may be composed
with other elemental functions to perform powerful computations,
especially when applied to arrays.  Our prototype tool transforms
elemental functions to inline OpenCL functions.  Thus there is no
penalty for usage of elemental functions and they provide a convient
mechanism to express algorithms in simpler segments.

%Also note that the plus and times operators shown in the data-statement
%example can be seen as an elemental functions, as plus and times are defined
%in terms of scalars but can be applied to whole arrays and can return an array
%result.

\subsubsection*{Pure procedures}

Pure procedures, like elemental functions, must be free of side
effects.  Unlike elemental functions that require arguments to have an
{\tt intent(in)} attribute, they may change the contents of array
arguments that are passed to them.  The absence of side effects
removes ordering constraints that could restrict the freedom of the
compiler to invoke pure functions out of order and possibly in parallel.
Procedures and functions of this sort are also common in pure
functional languages like Haskell, and are exploited by compilers in
order to emit parallel code automatically due to their suitability for
compiler-level analysis.

Since pure procedures don't have side effects they a candidates for
running on accelerators in OpenCL.  Currently our prototype tool
transforms pure procedures to OpenCL kernels that \emph{do not} call
other procedures, except for elemental functions.

\subsubsection*{Shift functions}

Many array-based algorithms require the same operation to be performed
on each element of the array using the value of that element and some small
set of neighboring cells. As described above, this can often be done with
shift operations.
Fortran provides a set of shifting operators that allow programmers to
define operations based on shifted arrays.  These intrinsic operators take
an array, a dimension, and the amount by which it should be shifted (using
the sign to indicate direction).  By defining operations on entire arrays
based on a global view of them shifted relative to each other, programmers can
avoid explicit looping and potentially tricky index arithmetic.  Furthermore,
analysis of the extent of the set of shifted arrays in a given expression
allows analysis tools to determine the amount of temporary or buffer storage
necessary to hold intermediate values during whole array operations.  With
explicit loops, programmers must maintain this temporary storage manually.

%% Deleted by CER
%% Often programmers implement these operations that are local to each element within the inner-loop of a set of nested FOR- or DO-loops using offsets relative to the current array index.  An alternative to this local-view of the algorithm is to take a global view and write the algorithm in terms of the whole array.  For example, consider a 1D array in which we wish to subtract the $(i-1)$th element from the $i$th for all elements.  One way to look at this is that we are subtracting the entire array shifted by one element from itself.

\subsection{New Procedures}

Unlike OpenCL, we require limited use of compiler directives, although
two are used to enforce the semantics required by the programming
model.  These directives are:

\begin{itemize}

\item {\tt \!\$OFP CONTIGUOUS}: specifies that an dummy array variable is
  contiguous in memory. CONTIGUOUS is an attribute in Fortran 2008.

\item {\tt \!\$OFP KERNEL}: specifies that a pure subroutine can be
  transformed to an OpenCL kernel.

\end{itemize}

Borrowing ideas from ZPL, we introduce a concept of a region to
Fortran with a set of functions that allow programmers to work with
subarrays in expressions.  In Fortran, these functions return a copy
of an existing array or array section.  This is unlike ZPL where
regions are analogous to index sets and are used primarily for address
resolution within an array without dictating storage related behavior.
The functions that we propose are similar in that they allow a
programmer to deal with index regions that are meaningful to their
algorithm, and automatically induce a halo (or ghost) cell pattern as
needed in the implementation generated by the compiler, where the size
of an array is increased to provide extra array elements surrounding
the interior portion of the array.  This is critical to reducing the
amount of code that the programmer is forced to write, as the halo cells
are often where boundary errors and off-by-one errors may occur in code
that is manually generated.

Region functions are similar to the shift operator as they can be used
to reference portions of the array that are shifted with respect to
the interior portion.  However, unlike the shift operator, regions are
not expressed in terms of boundary conditions and thus don't
explicitly \emph{require} a knowledge of nor the application of
boundary conditions locally.  Thus, as will be shown below, regions
are more suitable for usage by OpenCL thread groups which access only
local subsections of an array stored in global memory.

Three new procedures (in addition to the intrinsic shift function) are
defined in Fortran that are used in array-syntax operations.  Each
procedure takes a integer array halo argument that specifies the
number of ghost cells on either side of a region, for each dimension.
For example {\tt halo = [left, right, down, up]} specifies a halo for
a two-dimensional region.  These functions are:

\begin{itemize}

\item {\tt transfer\_halo(array, halo)}: an impure subroutine that
  exchanges halo regions between nodes using MPI or Fortran
  coarrays. Array is the memory stored on the node representing the
  node-local portion of a virtual global array. Not used in this work.

\item {\tt interior(array, halo)}: a pure function that returns a copy of
  the interior portion of the array specified by halo. Array is
  local to a node.

\item {\tt region(array, halo)}: a pure function that returns a copy of
  the portion of the array specified by halo.  Array is local to a node.

\end{itemize}

It should be noted that the two functions {\tt interior} and {\tt
  region} are pure and thus can be called from within a pure kernel
procedure.  These two functions are part of the language recognized by
the compiler and though the two functions return a copy of a portion
of an array \emph{semantically}, the compiler is not forced to
actually make a copy and is free to enforce copy semantics through
other means.

\subsection{Advantages}

There are several advantages to this style of programming using array
syntax, shifts, regions, and pure and elemental functions:

\begin{itemize}
\item There are no loops or index variables to keep track of.  Off by
  one index errors and improper handling of array boundaries are a
  common programming mistake.
\item The written code is closer to the algorithm, easier to
  understand, and is usually substantially shorter.
\item Semantically the intrinsic functions return arrays by value.
  This is usually what the algorithm requires.
\item Pure and elemental function are free from side effects, so it is
  easier for a compiler to schedule the work to be done in parallel.
\end{itemize}

%% MJS - mention that these features were designed for parallelism in
%% the first place?

%% Deleted by CER
%% An example of this style of programming in Fortran is shown in
%
%{\small
%\begin{verbatim}
%  Bz = Bz &
%     + dt*(cshift(Ex,dim=2,shift=+1)-Ex)/dy &
%     - dt*(cshift(Ey,dim=1,shift=+1)-Ey)/dx
%\end{verbatim}
%}

%%This example is a solution to Maxwell's equations for the $z$ component of the magnetic field using Fortran array syntax.  Note that there are no explicit loops in this example.  The operators {\tt +}, {\tt -}, and {\tt *} are applied to all of the elements of the three-dimensional arrays {\tt Bz}, {\tt Ex}, and {\tt Ey}, individually.

Data parallelism has been called
collection-oriented programming by
Blelloch~\cite{blelloch90,rajopadhye93}.  As the {\tt cshift}
function and the array-valued expressions all semantically returns a
value, this style of programming is also similar to functional
programming (or value-oriented programming). 

%%~\cite{simonpeytonjones}).  The heart of the solution
%to Maxwell's equations is the statement shown in
%Listing~\ref{lst:dpexample} and five similar, simple equations.

%% NEW - CER
%%% The power in this notation is that the compiler is free to distribute the computation in these expressions over any hardware processing elements available to it, such as the vector units on an Intel or AMD processor or the Synergistic Processing Units on an IBM Cell processor.  If the arrays are declared as coarrays, this includes spreading the computation over \emph{all of the nodes in   a cluster as well.}  In this case, communication occurs within the \texttt{cshift} functions, though the compiler is free to overlap communication and computation by scheduling the communication early and performing the computation on the interior of the arrays while waiting for the communication to complete.

%% NEW - CER
%Complete and very concise and elegant programs can be built with procedures
%similar to the example shown above. To aid this effort, Fortran
%supplies intrinsic functions like the array constructors (\texttt{CSHIFT},
%\texttt{EOSHIFT}, \texttt{MERGE}, \texttt{TRANSPOSE}, ...), the array location
%functions (\texttt{MAXLOC} and \texttt{MINLOC}), and the array reduction
%functions (\texttt{ANY}, \texttt{COUNT}, \texttt{MINVAL}, \texttt{SUM},
%\texttt{PRODUCT}, ...).  To this set we add region functions described above.

%% Deleted by CER
%% This style of programming meets the requirements we have set for a programming model for developing applications suitable for acceleration.  It allows the programmer to program at a very-high level of abstraction while providing the compiler with maximum flexibility in targeting the application for a particular hardware architecture.  The data parallel programming model simultaneously meets the seemingly conflicting goals of maintainability, portability, and performance.

Unfortunately, this style of programming has never really caught on
because when Fortran 90 was first introduced, performance of these
features was relatively poor and thus programmers shied away from
using array syntax (even recently, some are actively counseling against its
usage because of performance issues~\cite{Levesque:SC08}).  Thus the
Fortran community was caught in a classic ``chicken-and-egg''
conundrum: (1) programmers didn't use it because it was slow; and (2)
compilers vendors didn't improve it because programmers didn't use it.
A goal of this paper is to demonstrate that parallel programs written
in this style of Fortran are maintainable and can achieve good
performance on accelerator architectures.

\subsection{Current Limitations}

Only pure Fortran procedures are transformed into OpenCL kernels.  The
programmer must explicitly call these kernels from Fortran using the
ForOpenCL library described below.  It is also possible, using ROSE,
to modify the calling site so that the entire program can be
transformed, but this functionality is outside the scope of this
paper.  Here we specifically examine transforming Fortran procedures
to OpenCL kernels.  Because OpenCL support is relatively new to ROSE,
some generated code must be modified.  For example, the {\tt
  \_\_global} attribute for kernel arguments was added by hand.

It is assumed that memory for all arrays reside on the device.  The
programmer must copy memory to and from the device.  In addition,
array size (neglecting ghost cell regions) must be multiples of the
global OpenCL kernel size.

Only elemental functions may be called from kernel functions.  These
include Fortran functions that have an OpenCL analog and user-defined
elemental functions.

A kernel procedure (specified by the {\tt \!\$OFP KERNEL} directive) must be
pure and all array variables must be declared as contiguous.  A kernel
procedure may not call other procedures except for limited intrisic
functions (primarily math), user-defined elemental function, and the
{\tt interior} and {\tt region} functions.  We plan to address
non-contiguous arrays (such as those that result from strided access)
by mapping array strides to gather/scatter-style memory accessors.
